{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2272,"status":"ok","timestamp":1718482991592,"user":{"displayName":"강서현","userId":"07897779369928704314"},"user_tz":-540},"id":"Q4_l6Ph8H3iA","outputId":"6c62db5c-e2f2-4e38-f990-76fd4fcfa327"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1718482991592,"user":{"displayName":"강서현","userId":"07897779369928704314"},"user_tz":-540},"id":"rD80s9v-H-Nw"},"outputs":[],"source":["HOME = '/content/drive/MyDrive/AnimalVITON/OOTDiffusion'"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1718482991593,"user":{"displayName":"강서현","userId":"07897779369928704314"},"user_tz":-540},"id":"dlQnOs9pIA7k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2f134580-fb47-44c5-e230-4464ed0bc563"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/AnimalVITON/OOTDiffusion/OOTDiffusion/run\n"]}],"source":["%cd {HOME}/OOTDiffusion/run"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":11362,"status":"ok","timestamp":1718483002951,"user":{"displayName":"강서현","userId":"07897779369928704314"},"user_tz":-540},"id":"eoXJYMNfIMJd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"142540ab-b615-41d9-9128-c7d37e9531af"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --quiet\n","!pip install -r requirements.txt --quiet\n","!pip install ultralytics --quiet"]},{"cell_type":"code","source":["!pip install config einops onnxruntime diffusers==0.24.0 accelerate==0.26.1 --quiet"],"metadata":{"id":"9JwPpmMGcYxn","executionInfo":{"status":"ok","timestamp":1718483008476,"user_tz":-540,"elapsed":5532,"user":{"displayName":"강서현","userId":"07897779369928704314"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# 새로운 이미지 추가시: 이미지 형식+해상도 맞추기\n","import os\n","from PIL import Image\n","\n","def convert_and_resize_images(folder, target_size=(768, 1024)):\n","    for root, dirs, files in os.walk(folder):\n","        for file in files:\n","            try:\n","                file_path = os.path.join(root, file)\n","\n","                with Image.open(file_path) as img:\n","                    img = img.convert(\"RGB\")\n","                    img = img.resize(target_size, Image.ANTIALIAS)\n","                    new_file_path = os.path.splitext(file_path)[0] + \".jpg\"\n","                    img.save(new_file_path, \"JPEG\")\n","                if file_path != new_file_path:\n","                    os.remove(file_path)\n","\n","            except Exception as e:\n","                print(f\"Error processing file {file_path}: {e}\")\n","\n","convert_and_resize_images('examples/model')\n","convert_and_resize_images('examples/garment')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lNNHA5F5lahi","executionInfo":{"status":"ok","timestamp":1718483009642,"user_tz":-540,"elapsed":1186,"user":{"displayName":"강서현","userId":"07897779369928704314"}},"outputId":"121add04-9169-45f9-dc41-e1421ac8d32b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-14-c4cbad78f25b>:13: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n","  img = img.resize(target_size, Image.ANTIALIAS)\n"]}]},{"cell_type":"code","source":["!python run_ootd.py --model_path examples/model/model_2.jpg --cloth_path examples/garment/garment_1.jpg --scale 2.0 --sample 4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n1svZv8QkLTz","executionInfo":{"status":"ok","timestamp":1718483155587,"user_tz":-540,"elapsed":44182,"user":{"displayName":"강서현","userId":"07897779369928704314"}},"outputId":"f0eb7377-bce4-40c4-d62c-6ef8da7040de"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\rLoading pipeline components...:   0% 0/7 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.23it/s]\n","START YOLO INFERENCING...\n","\n","image 1/1 /content/drive/MyDrive/AnimalVITON/OOTDiffusion/OOTDiffusion/run/examples/model/model_2.jpg: 800x608 1 cloth, 81.7ms\n","Speed: 7.2ms preprocess, 81.7ms inference, 3.8ms postprocess per image at shape (1, 3, 800, 608)\n","FINISH YOLO INFERENCING...\n","Initial seed: 232152705\n","100% 20/20 [00:15<00:00,  1.28it/s]\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNKDZbGViDYmqKOibIce2Aq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}